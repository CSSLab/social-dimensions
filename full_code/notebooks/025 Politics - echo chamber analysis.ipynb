{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paths\n",
    "from commembed.jupyter import *\n",
    "import commembed.linalg as linalg\n",
    "import commembed.dimens as dimens\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import commembed.data as data\n",
    "spark = data.spark_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = load_embedding('reddit', 'master')\n",
    "dimen_list = dimens.load_dimen_list('final')\n",
    "\n",
    "def create_nullhyp_counts():\n",
    "    nullhyp_data_path = \"~/reddit_nullhyp/train_set_unshuffled.csv.cat\"\n",
    "    nullhyp_data = spark.read.csv(nullhyp_data_path, schema='subreddit STRING, author STRING', sep=' ')\n",
    "    \n",
    "    spark.sql(\"\"\"select author, subreddit, count(*) as num_comments\n",
    "          from nullhyp_data group by 1, 2\"\"\") \\\n",
    "        .write.parquet(\"/ada/data/reddit/parquet/all_objects_nullhyp_user_counts.parquet\")\n",
    "    \n",
    "    nullhyp_embedding = load_embedding('reddit', 'nullhyp')\n",
    "    copycat_dimen_list = dimens.load_copycat_dimen_list('final', nullhyp_embedding)\n",
    "    nullhyp_scores = dimens.score_embedding(nullhyp_embedding, copycat_dimen_list)\n",
    "    \n",
    "    scores = dimens.score_embedding(embedding, dimen_list)\n",
    "    scores = scores.apply(lambda x: (x - np.mean(x))/np.std(x), axis=0)\n",
    "    nullhyp_scores = nullhyp_scores.apply(lambda x: (x - np.mean(x))/np.std(x), axis=0)\n",
    "    \n",
    "    for partisan_dimen in [\"partisan\", \"partisan_b\"]:\n",
    "    \n",
    "        neutral_cutoff = neutral_cutoffs\n",
    "\n",
    "        scores_subset = scores[scores[partisan_dimen+'_neutral'] > neutral_cutoff[partisan_dimen]]\n",
    "        print(\"%d political subreddits selected\" % len(scores_subset))\n",
    "\n",
    "        nullhyp_scores_subset = nullhyp_scores.sort_values(partisan_dimen+'_neutral', ascending=False).iloc[:len(scores_subset)]\n",
    "        print(\"%d nullhyp political subreddits selected\" % len(nullhyp_scores_subset))\n",
    "        display(nullhyp_scores_subset.sort_values(partisan_dimen).head(5))\n",
    "\n",
    "        plt.hist(nullhyp_scores[partisan_dimen], bins=100, label='all')\n",
    "        plt.hist(nullhyp_scores_subset[partisan_dimen], bins=100, label='political')\n",
    "        plt.title(partisan_dimen)\n",
    "        plt.show()\n",
    "        plt.hist(scores[partisan_dimen], bins=100, label='all')\n",
    "        plt.hist(scores_subset[partisan_dimen], bins=100, label='political')\n",
    "        plt.title(partisan_dimen)\n",
    "        plt.show()\n",
    "\n",
    "        spark.createDataFrame(nullhyp_scores_subset.reset_index()) \\\n",
    "            .createOrReplaceTempView(\"scores\")\n",
    "\n",
    "        result = spark.sql(\"\"\"\n",
    "\n",
    "            select author, subreddit, count(*) as num_comments\n",
    "\n",
    "            from nullhyp_data\n",
    "\n",
    "            inner join scores\n",
    "            on subreddit = community\n",
    "\n",
    "            group by 1, 2\n",
    "        \"\"\")\n",
    "        result.write.parquet(data.DATA_PATH+\"/all_objects_nullhyp_user_counts_filt_%s.parquet\" % partisan_dimen, mode='overwrite')\n",
    "    \n",
    "#create_nullhyp_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partisan_dimen = \"partisan\"\n",
    "nullhyp = True\n",
    "\n",
    "data_suffix = f\"_filt_{partisan_dimen}\" if partisan_dimen is not None else \"\"\n",
    "user_counts_table = (\"all_objects_nullhyp_user_counts\" if nullhyp else \"all_objects_user_counts\") + data_suffix\n",
    "df = data.load(user_counts_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if partisan_dimen is not None:\n",
    "    scores_z, _ = load_politics_z_df(partisan_dimen, \"all\")\n",
    "else:\n",
    "    if nullhyp:\n",
    "        embedding = load_embedding('reddit', 'nullhyp')\n",
    "        dimen_list = dimens.load_copycat_dimen_list('final', embedding)\n",
    "    else:\n",
    "        embedding = load_embedding('reddit', 'master')\n",
    "        dimen_list = dimens.load_dimen_list('final', embedding)\n",
    "        \n",
    "    \n",
    "    scores = dimens.score_embedding(embedding, dimen_list)\n",
    "    scores_z = scores.apply(lambda x: (x - np.mean(x))/np.std(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_bins = 5\n",
    "if (num_bins % 2) != 1:\n",
    "    raise \"num_bins must be odd to accomodate center bin\"\n",
    "\n",
    "breadth = 2\n",
    "\n",
    "bins_per_side = (num_bins - 1) / 2\n",
    "bin_size = breadth / bins_per_side\n",
    "bins = np.arange(1, bins_per_side+1) * bin_size\n",
    "\n",
    "# remember the double-width center bin\n",
    "bin_labels = np.array(np.flip(bins*-1).tolist() + [0] + bins.tolist())\n",
    "bins = np.array(np.flip(bins*-1).tolist() +  bins.tolist())\n",
    "\n",
    "\n",
    "scores_bin = scores_z.apply(lambda x: bin_labels[np.digitize(x,bins)], axis=0)\n",
    "\n",
    "scores_bin_orig = scores_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If nullhyp, create a new distribution of scores that has the same number of communities as the real kind\n",
    "if nullhyp and partisan_dimen is not None:\n",
    "    \n",
    "    nullhyp_embedding = load_embedding('reddit', 'nullhyp')\n",
    "    copycat_dimen_list = dimens.load_copycat_dimen_list('final', nullhyp_embedding)\n",
    "    nullhyp_scores = dimens.score_embedding(nullhyp_embedding, copycat_dimen_list)\n",
    "\n",
    "    # Select the n most political subs from the nullhyp, where n is the number of political subs in real life\n",
    "    nullhyp_scores_bin = nullhyp_scores.sort_values(partisan_dimen+\"_neutral\", ascending=False).iloc[0:len(scores_bin)]\n",
    "    nullhyp_scores_bin = nullhyp_scores_bin[[partisan_dimen]].rename(columns={partisan_dimen:\"partisan_dimen\"})\n",
    "    nullhyp_scores_bin = nullhyp_scores_bin.sort_values(\"partisan_dimen\").reset_index()\n",
    "    \n",
    "    # Assign the same bin to communities with the same index, so the ordering/amount of subs in bin is preserved\n",
    "    nullhyp_scores_bin = (nullhyp_scores_bin.join(scores_bin_orig.sort_values(\"partisan_dimen\").reset_index(), rsuffix='_actual'))\n",
    "    \n",
    "    plt.hist(nullhyp_scores_bin[\"partisan_dimen_actual\"])\n",
    "    \n",
    "    scores_bin = nullhyp_scores_bin[['community', 'partisan_dimen_actual']].rename(columns={\"partisan_dimen_actual\": \"partisan_bin\"}).set_index(\"community\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(scores_bin.reset_index().melt(id_vars=[\"community\"], value_name=\"bin\", var_name=\"dimen\")) \\\n",
    "    .createOrReplaceTempView(\"scores_bin\")\n",
    "\n",
    "user_bin_freq = spark.sql(f\"\"\"\n",
    "\n",
    "    select author, dimen, bin, sum(num_comments) as num_comments\n",
    "    \n",
    "    from {user_counts_table}\n",
    "    \n",
    "    inner join scores_bin\n",
    "    on subreddit = community\n",
    "\n",
    "    group by 1, 2, 3\n",
    "\"\"\")\n",
    "base = \"user_dists_nullhyp\" if nullhyp else \"user_dist\"\n",
    "user_bin_freq.write.parquet(\"/ada/data/reddit/parquet/echo_chambers/%s_bins=%s_breadth=%s_%s.parquet\" % (base, num_bins, breadth, data_suffix),\n",
    "                           mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_suffix = \"_filt_partisan\"\n",
    "nullhyp=False\n",
    "\n",
    "addtl = \"_nullhyp\" if nullhyp else \"\"\n",
    "user_dists_table = f\"user_dists{addtl}_bins=5_breadth=2_\" + data_suffix\n",
    "\n",
    "spark.read.parquet(\"/ada/data/reddit/parquet/echo_chambers/\" + user_dists_table + \".parquet\") \\\n",
    "    .filter(col(\"author\") != \"[deleted]\") \\\n",
    "    .createOrReplaceTempView(\"user_dists\")\n",
    "\n",
    "totals = spark.sql(\"select author, dimen, sum(num_comments) as total_comments from user_dists group by 1, 2\").cache()\n",
    "totals.createOrReplaceTempView(\"totals\")\n",
    "\n",
    "bin_totals = spark.sql(\"select bin, dimen, sum(num_comments) as total_comments from user_dists group by 1, 2\").cache()\n",
    "bin_totals.createOrReplaceTempView(\"bin_totals\")\n",
    "\n",
    "bin_totals.toPandas().to_csv('echo_chamber_bin_sizes_%s.csv' % user_dists_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(f\"\"\"\n",
    "    select fracs2.dimen, bin1, bin2, weighted_bin2_frac/bin_totals.total_comments as bin1_avg_frac_bin2,\n",
    "        weighted_bin2_comments/bin_totals.total_comments as bin1_avg_bin2_comments\n",
    "    from (\n",
    "    select dimen, bin1, bin2, sum(bin1_comments*bin2_frac) as weighted_bin2_frac,\n",
    "        sum(bin1_comments*bin2_comments) as weighted_bin2_comments\n",
    "    \n",
    "    from (\n",
    "        select s1.author, s1.dimen, s1.bin as bin1, s2.bin as bin2,\n",
    "            s1.num_comments as bin1_comments,\n",
    "            s2.num_comments as bin2_comments,\n",
    "            (s2.num_comments/total_comments) as bin2_frac\n",
    "            \n",
    "        from user_dists as s1\n",
    "\n",
    "        inner join user_dists as s2\n",
    "        on s1.author = s2.author and s1.dimen = s2.dimen\n",
    "\n",
    "        inner join totals\n",
    "        on s1.author = totals.author and s1.dimen = totals.dimen\n",
    "    ) fracs\n",
    "    \n",
    "    group by 1, 2, 3\n",
    "    ) fracs2\n",
    "    \n",
    "    inner join bin_totals\n",
    "    on bin_totals.bin = bin1 and bin_totals.dimen = fracs2.dimen\n",
    "\"\"\").toPandas()\n",
    "result.to_csv(\"echo_chamber_results_%s.csv\" % user_dists_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per community version\n",
    "sub_totals = spark.sql(\"select subreddit, sum(num_comments) as total_comments from user_counts group by 1\").cache()\n",
    "sub_totals.createOrReplaceTempView(\"sub_totals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(f\"\"\"\n",
    "    select fracs2.dimen, fracs2.subreddit, bin2, weighted_bin2_frac/sub_totals.total_comments as subreddit_avg_frac_bin2\n",
    "    from (\n",
    "    select dimen, subreddit, bin2, sum(sub_comments*bin2_frac) as weighted_bin2_frac\n",
    "    \n",
    "    from (\n",
    "        select user_counts.author, s2.dimen, subreddit, s2.bin as bin2,\n",
    "            user_counts.num_comments as sub_comments, (s2.num_comments/total_comments) as bin2_frac\n",
    "            \n",
    "        from user_counts\n",
    "\n",
    "        inner join user_dists as s2\n",
    "        on user_counts.author = s2.author\n",
    "\n",
    "        inner join totals\n",
    "        on s2.author = totals.author and s2.dimen = totals.dimen\n",
    "    ) fracs\n",
    "    \n",
    "    group by 1, 2, 3\n",
    "    ) fracs2\n",
    "    \n",
    "    inner join sub_totals\n",
    "    on sub_totals.subreddit = fracs2.subreddit\n",
    "\"\"\").toPandas()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_subs = sub_totals.toPandas()\n",
    "\n",
    "largest_subs = largest_subs.set_index(\"subreddit\")\n",
    "result = result.join(largest_subs, on=\"subreddit\", how=\"inner\")\n",
    "embedding = load_embedding('reddit', 'master')\n",
    "result = result[result[\"subreddit\"].isin(embedding.vectors.index)]\n",
    "\n",
    "result.to_parquet(\"echo_chamber_community_results_%s.parquet\" % user_dists_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commembed",
   "language": "python",
   "name": "commembed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
