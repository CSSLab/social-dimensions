{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate data for the polarization levels over time of yearly cohorts.\n",
    "* Can look at all political activity or just left, right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "partisan_dimen = \"partisan\"\n",
    "political_activity_category = \"all\" # can be 'all_unfiltered' to include all activity, including apolitical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark WebUI: http://ada.ais.sandbox:4046\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from commembed.jupyter import *\n",
    "import commembed.linalg as linalg\n",
    "import commembed.dimens as dimens\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import commembed.data as data\n",
    "import commembed.analysis as analysis\n",
    "\n",
    "\n",
    "spark = data.spark_context()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark WebUI: http://ada.ais.sandbox:4046\n",
      "10006 political subreddits selected\n"
     ]
    }
   ],
   "source": [
    "user_counts = data.load(\"all_objects_monthly_user_counts\")\n",
    "user_counts.createOrReplaceTempView(\"user_counts\")\n",
    "\n",
    "scores_filtered, scores_df = load_abs_z_df(partisan_dimen, political_activity_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_counts = spark.sql(\"select * from user_counts inner join scores on subreddit = community\")\n",
    "filtered_counts.createOrReplaceTempView(\"filtered_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_first_months = filtered_counts.groupBy(\"author\").agg(min(\"month\").alias(\"first_month\"))\n",
    "author_first_months.cache()\n",
    "author_first_months.createOrReplaceTempView(\"author_first_months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_month = \"2012-01\"\n",
    "\n",
    "# Can first aggregate by subreddit since score is constant for a given subreddit\n",
    "subreddit_aggregated_rows = spark.sql(f\"\"\"\n",
    "    select substr(greatest(first_month, '{min_month}'), 0, 4) as first_year, month, subreddit,\n",
    "        sum(num_comments) as num_comments,\n",
    "        first(partisan_dimen) as partisan_dimen\n",
    "        \n",
    "    from filtered_counts\n",
    "    \n",
    "    left join author_first_months\n",
    "    on author_first_months.author = filtered_counts.author\n",
    "    \n",
    "    where filtered_counts.author != \"[deleted]\"\n",
    "    \n",
    "    group by 1, 2, 3\n",
    "\"\"\")\n",
    "subreddit_aggregated_rows.cache()\n",
    "subreddit_aggregated_rows.createOrReplaceTempView(\"subreddit_aggregated_rows\")\n",
    "\n",
    "means = spark.sql(f\"\"\"\n",
    "    select first_year, month, sum(num_comments*partisan_dimen)/sum(num_comments) as avg_abs_z_score\n",
    "    from subreddit_aggregated_rows\n",
    "    group by 1, 2\n",
    "\"\"\")\n",
    "means.cache()\n",
    "means.createOrReplaceTempView(\"means\")\n",
    "\n",
    "user_cohorts = spark.sql(\"\"\"\n",
    "    select subreddit_aggregated_rows.first_year,\n",
    "            subreddit_aggregated_rows.month, \n",
    "            sum(num_comments) as num_comments,\n",
    "            first(means.avg_abs_z_score) as avg_abs_z_score,\n",
    "            sqrt(sum(pow(partisan_dimen-avg_abs_z_score, 2)*num_comments)/sum(num_comments)) as std_dev\n",
    "        \n",
    "    from subreddit_aggregated_rows\n",
    "    \n",
    "    inner join means\n",
    "    on means.first_year = subreddit_aggregated_rows.first_year\n",
    "        and means.month = subreddit_aggregated_rows.month\n",
    "        \n",
    "    group by 1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also calculate population level standard deviation\n",
    "overall_means = spark.sql(\"\"\"\n",
    "    select month, sum(num_comments*partisan_dimen)/sum(num_comments) as avg_abs_z_score\n",
    "    from subreddit_aggregated_rows\n",
    "    group by 1\n",
    "\"\"\")\n",
    "overall_means.cache()\n",
    "overall_means.createOrReplaceTempView(\"overall_means\")\n",
    "\n",
    "overall_stats = spark.sql(\"\"\"\n",
    "    select subreddit_aggregated_rows.month, \n",
    "            sum(num_comments) as num_comments,\n",
    "            first(overall_means.avg_abs_z_score) as avg_abs_z_score,\n",
    "            sqrt(sum(pow(partisan_dimen-avg_abs_z_score, 2)*num_comments)/sum(num_comments)) as std_dev\n",
    "        \n",
    "    from subreddit_aggregated_rows\n",
    "    \n",
    "    inner join overall_means\n",
    "    on overall_means.month = subreddit_aggregated_rows.month\n",
    "        \n",
    "    group by 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "user_cohorts.write.parquet(\n",
    "    os.path.join(data.DATA_PATH, \"cohort_data_%s_%s.parquet\" % (partisan_dimen, political_activity_category)),\n",
    "    mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "overall_stats.write.parquet(\n",
    "    os.path.join(data.DATA_PATH, \"cohort_data_overall_%s_%s.parquet\" % (partisan_dimen, political_activity_category)),\n",
    "    mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condakernel",
   "language": "python",
   "name": "condakernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
