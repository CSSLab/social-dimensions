{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from commembed.jupyter import *\n",
    "import commembed.linalg as linalg\n",
    "import commembed.dimens as dimens\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "#import commembed.data as data\n",
    "#spark = data.spark_context()\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Freshly loading table all_objects)\n"
     ]
    }
   ],
   "source": [
    "all_objects = data.load(\"all_objects\")\n",
    "# all comments before 2019\n",
    "#spark.sql(\"select count(*) from all_objects where id like 't1_%' and created_utc < 1546369200\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------------------+-------------------+\n",
      "|total_comments|distinct_authors|         first_date|          last_date|\n",
      "+--------------+----------------+-------------------+-------------------+\n",
      "|    5075175045|        34665723|2005-12-12 00:26:28|2018-12-31 18:59:59|\n",
      "+--------------+----------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select count(*) as total_comments,\n",
    "    count(distinct author) as distinct_authors,\n",
    "    from_unixtime(min(created_utc), 'yyyy-MM-dd HH:mm:ss') as first_date,\n",
    "    from_unixtime(max(created_utc), 'yyyy-MM-dd HH:mm:ss') as last_date from all_objects\n",
    "    where id like 't1_%' and created_utc < 1546300800\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = load_embedding('reddit', 'master')\n",
    "dimen_list = dimens.load_dimen_list('final')\n",
    "scores = dimens.score_embedding(embedding, dimen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pctl = scores.apply(lambda x: np.digitize(x, np.percentile(x, np.arange(1, 100))), axis=0)\n",
    "scores_z = scores.apply(lambda x: (x-np.mean(x))/np.std(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_z.loc[\"progressive\"])\n",
    "print(scores_z.loc[\"LesbianGamers\"])\n",
    "print(scores_pctl.loc[\"cycling\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# total numbers / embedding cutoff numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_user_counts = data.load(\"all_objects_monthly_user_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+----------+\n",
      "|sum(num_comments)|min(month)|max(month)|\n",
      "+-----------------+----------+----------+\n",
      "|       5075996316|   2005-12|   2018-12|\n",
      "+-----------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify integrity\n",
    "spark.sql(\"select sum(num_comments), min(month), max(month) from all_objects_monthly_user_counts where month < '2019-01'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_totals = spark.sql(\"\"\"\n",
    "    select month, sum(num_comments) as total_comments\n",
    "    from all_objects_monthly_user_counts\n",
    "    where month < '2019-01'\n",
    "    group by 1\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores_df = spark.createDataFrame(scores.reset_index())\n",
    "scores_df.createOrReplaceTempView(\"scores\")\n",
    "counts_w_sub = spark.sql(\"\"\"\n",
    "    select author, subreddit, cast((gender is not null) as int) as in_embedding, num_comments\n",
    "    from all_objects_monthly_user_counts\n",
    "    \n",
    "    left join scores\n",
    "    on scores.community = subreddit\n",
    "    \n",
    "    where month < '2019-01'\n",
    "\"\"\")\n",
    "counts_w_sub.createOrReplaceTempView(\"counts_w_sub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|pct_comments_in_embedding|\n",
      "+-------------------------+\n",
      "|       0.9537859152378471|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Percent of comments included in embedding\n",
    "spark.sql(\"select (sum(in_embedding*num_comments)/sum(num_comments)) as pct_comments_in_embedding from counts_w_sub\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|pct_users_in_embedding|\n",
      "+----------------------+\n",
      "|    0.9316604452842877|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Percent of users included in embedding (at least 1 comment)\n",
    "spark.sql(\"select avg(in_embedding) as pct_users_in_embedding from (select max(in_embedding) as in_embedding from counts_w_sub group by author)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|pct_users_in_embedding|\n",
      "+----------------------+\n",
      "|    0.6895200859253661|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Percent of users included in embedding (all comments)\n",
    "spark.sql(\"select avg(in_embedding) as pct_users_in_embedding from (select min(in_embedding) as in_embedding from counts_w_sub group by author)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sub_counts = spark.sql(\"\"\"\n",
    "    select author, count(distinct subreddit) as sub_count\n",
    "    from all_objects_monthly_user_counts\n",
    "    where month < '2019-01'\n",
    "    group by 1\n",
    "\"\"\").cache()\n",
    "user_sub_counts.createOrReplaceTempView(\"user_sub_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|   avg(sub_count)|\n",
      "+-----------------+\n",
      "|9.613033611732035|\n",
      "+-----------------+\n",
      "\n",
      "+------------------+\n",
      "|   proportion_gt_1|\n",
      "+------------------+\n",
      "|0.5285568330137629|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select avg(sub_count) from user_sub_counts\").show()\n",
    "spark.sql(\"select avg(cast(sub_count > 1 as int)) as proportion_gt_1 from user_sub_counts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# appendix: input pairs and discovered pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_pair(a, b):\n",
    "    a = a.replace(\"_\", \"\\\\_\")\n",
    "    b = b.replace(\"_\", \"\\\\_\")\n",
    "    \n",
    "    return \"\\\\textsf{r/%s}& $\\\\,\\\\to\\\\,$ & \\\\textsf{r/%s}\" % (b, a)\n",
    "\n",
    "to_print = [\n",
    "    \"age\",\"gender\",\"partisan\",\"affluence\",\"age_b\",\"gender_b\",\"partisan_b\",\n",
    "    'sociality', 'edginess', 'time'\n",
    "]\n",
    "\n",
    "out = \"\"\n",
    "glossary = []\n",
    "\n",
    "is_first = True\n",
    "axes_by_name = {d[0]:d[1] for d in dimen_list}\n",
    "for name in to_print:\n",
    "    data = axes_by_name[name]\n",
    "        \n",
    "    seeds = data['seeds']\n",
    "    positive_comms = data['positive_comms']\n",
    "    negative_comms = data['negative_comms']\n",
    "    glossary.extend(positive_comms)\n",
    "    glossary.extend(negative_comms)\n",
    "    \n",
    "    pairs = [format_pair(a, b) for a, b in zip(positive_comms, negative_comms)]\n",
    "    pairs = pairs[1:] # chop off seed\n",
    "    \n",
    "    midpoint = int(len(pairs) / 3)\n",
    "    rows = [\"%s & %s & %s \\\\\\\\\" % (a, b, c) for a, b, c in [pairs[0:3], pairs[3:6], pairs[6:]]]\n",
    "    tbody = \"\\n\".join(rows)\n",
    "    if not is_first:\n",
    "        out += (\"\\\\hline\")\n",
    "    is_first = False\n",
    "    out += (r\"\"\"\n",
    "    \n",
    "\\multicolumn{6}{c}{\\normalsize{\\dimension{%s}}} &\n",
    " \\cellcolor{blue!25} r/%s & \\cellcolor{blue!25}$\\,\\to\\,$ & \\cellcolor{blue!25} r/%s \\\\\n",
    " \n",
    "%s\n",
    "    \"\"\" % (axis_name(name), seeds[0][0].replace(\"_\", \"\\\\_\"), seeds[0][1].replace(\"_\", \"\\\\_\"), tbody))\n",
    "    \n",
    "with open(\"../paper_resources/seeds.tex\", \"w\") as text_file:\n",
    "    text_file.write(out)\n",
    "with open(\"../paper_resources/glossary_seeds.txt\", \"w\") as text_file:\n",
    "    text_file.write(\"\\n\".join(glossary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condakernel",
   "language": "python",
   "name": "condakernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
